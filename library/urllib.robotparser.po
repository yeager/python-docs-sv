# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2001 Python Software Foundation
# This file is distributed under the same license as the Python package.
# FIRST AUTHOR <EMAIL@ADDRESS>, YEAR.
#
# Translators:
# python-doc bot, 2025
# Daniel Nylander <po@danielnylander.se>, 2025
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: Python 3.14\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2025-08-11 14:20+0000\n"
"PO-Revision-Date: 2025-08-02 17:35+0000\n"
"Last-Translator: Daniel Nylander <po@danielnylander.se>, 2025\n"
"Language-Team: Swedish (https://app.transifex.com/python-doc/teams/5390/"
"sv/)\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Language: sv\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"

msgid ":mod:`!urllib.robotparser` ---  Parser for robots.txt"
msgstr ":mod:`!urllib.robotparser` --- Parser för robots.txt"

msgid "**Source code:** :source:`Lib/urllib/robotparser.py`"
msgstr "**Källkod:** :source:`Lib/urllib/robotparser.py`"

msgid ""
"This module provides a single class, :class:`RobotFileParser`, which answers "
"questions about whether or not a particular user agent can fetch a URL on "
"the web site that published the :file:`robots.txt` file.  For more details "
"on the structure of :file:`robots.txt` files, see http://www.robotstxt.org/"
"orig.html."
msgstr ""
"Den här modulen tillhandahåller en enda klass, :class:`RobotFileParser`, som "
"svarar på frågor om huruvida en viss användaragent kan hämta en URL på den "
"webbplats som publicerade filen :file:`robots.txt`.  För mer information om "
"strukturen i :file:`robots.txt`-filer, se http://www.robotstxt.org/orig.html."

msgid ""
"This class provides methods to read, parse and answer questions about the :"
"file:`robots.txt` file at *url*."
msgstr ""
"Den här klassen innehåller metoder för att läsa, analysera och svara på "
"frågor om filen :file:`robots.txt` på *url*."

msgid "Sets the URL referring to a :file:`robots.txt` file."
msgstr "Ställer in URL:en som hänvisar till en :file:`robots.txt`-fil."

msgid "Reads the :file:`robots.txt` URL and feeds it to the parser."
msgstr "Läser URL:en :file:`robots.txt` och matar den till parsern."

msgid "Parses the lines argument."
msgstr "Analyserar argumentet lines."

msgid ""
"Returns ``True`` if the *useragent* is allowed to fetch the *url* according "
"to the rules contained in the parsed :file:`robots.txt` file."
msgstr ""
"Returnerar ``True`` om *useragent* får hämta *url* enligt de regler som "
"finns i den analyserade filen :file:`robots.txt`."

msgid ""
"Returns the time the ``robots.txt`` file was last fetched.  This is useful "
"for long-running web spiders that need to check for new ``robots.txt`` files "
"periodically."
msgstr ""
"Returnerar den tidpunkt då filen ``robots.txt`` senast hämtades.  Detta är "
"användbart för långkörande webbspindlar som behöver kontrollera om det finns "
"nya ``robots.txt``-filer med jämna mellanrum."

msgid ""
"Sets the time the ``robots.txt`` file was last fetched to the current time."
msgstr ""
"Ställer in tidpunkten för när filen ``robots.txt`` senast hämtades till "
"aktuell tid."

msgid ""
"Returns the value of the ``Crawl-delay`` parameter from ``robots.txt`` for "
"the *useragent* in question.  If there is no such parameter or it doesn't "
"apply to the *useragent* specified or the ``robots.txt`` entry for this "
"parameter has invalid syntax, return ``None``."
msgstr ""
"Returnerar värdet på parametern ``Crawl-delay`` från ``robots.txt`` för "
"*användaragenten* i fråga.  Om det inte finns någon sådan parameter eller om "
"den inte gäller för den angivna *användaragenten* eller om posten i ``robots."
"txt`` för denna parameter har ogiltig syntax, returneras ``None``."

msgid ""
"Returns the contents of the ``Request-rate`` parameter from ``robots.txt`` "
"as a :term:`named tuple` ``RequestRate(requests, seconds)``. If there is no "
"such parameter or it doesn't apply to the *useragent* specified or the "
"``robots.txt`` entry for this parameter has invalid syntax, return ``None``."
msgstr ""
"Returnerar innehållet i parametern ``Request-rate`` från ``robots.txt`` som "
"en :term:`named tuple`` ``RequestRate(requests, seconds)``. Om det inte "
"finns någon sådan parameter eller om den inte gäller för den *användaragent* "
"som anges eller om posten i ``robots.txt`` för denna parameter har ogiltig "
"syntax, returneras ``None``."

msgid ""
"Returns the contents of the ``Sitemap`` parameter from ``robots.txt`` in the "
"form of a :func:`list`. If there is no such parameter or the ``robots.txt`` "
"entry for this parameter has invalid syntax, return ``None``."
msgstr ""
"Returnerar innehållet i parametern ``Sitemap`` från ``robots.txt`` i form av "
"en :func:`list`. Om det inte finns någon sådan parameter eller om posten i "
"``robots.txt`` för denna parameter har ogiltig syntax, returneras ``None``."

msgid ""
"The following example demonstrates basic use of the :class:`RobotFileParser` "
"class::"
msgstr ""
"Följande exempel visar grundläggande användning av klassen :class:"
"`RobotFileParser`::"

msgid ""
">>> import urllib.robotparser\n"
">>> rp = urllib.robotparser.RobotFileParser()\n"
">>> rp.set_url(\"http://www.musi-cal.com/robots.txt\")\n"
">>> rp.read()\n"
">>> rrate = rp.request_rate(\"*\")\n"
">>> rrate.requests\n"
"3\n"
">>> rrate.seconds\n"
"20\n"
">>> rp.crawl_delay(\"*\")\n"
"6\n"
">>> rp.can_fetch(\"*\", \"http://www.musi-cal.com/cgi-bin/search?"
"city=San+Francisco\")\n"
"False\n"
">>> rp.can_fetch(\"*\", \"http://www.musi-cal.com/\")\n"
"True"
msgstr ""
">>> import urllib.robotparser\n"
">>> rp = urllib.robotparser.RobotFileParser()\n"
">>> rp.set_url(\"http://www.musi-cal.com/robots.txt\")\n"
">>> rp.read()\n"
">>> rrate = rp.request_rate(\"*\")\n"
">>> rrate.förfrågningar\n"
"3\n"
">>> rrate.sekunder\n"
"20\n"
">>> rp.crawl_delay(\"*\")\n"
"6\n"
">>> rp.can_fetch(\"*\", \"http://www.musi-cal.com/cgi-bin/search?"
"city=San+Francisco\")\n"
"Falsk\n"
">>> rp.can_fetch(\"*\", \"http://www.musi-cal.com/\")\n"
"True"

msgid "WWW"
msgstr "WWW"

msgid "World Wide Web"
msgstr "World Wide Web"

msgid "URL"
msgstr "URL"

msgid "robots.txt"
msgstr "robots.txt"
